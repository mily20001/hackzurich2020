{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2147483647"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FILE_NAME = 'sentiments.json'\n",
    "CSV_FOLDER = 'data'\n",
    "csv.field_size_limit(2147483647)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_dict_extract(key, var):\n",
    "    if hasattr(var, 'items'):\n",
    "        for k, v in var.items():\n",
    "            if k == key:\n",
    "                yield v\n",
    "            if isinstance(v, dict):\n",
    "                for result in gen_dict_extract(key, v):\n",
    "                    yield result\n",
    "            elif isinstance(v, list):\n",
    "                for d in v:\n",
    "                    for result in gen_dict_extract(key, d):\n",
    "                        yield result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv( path ):\n",
    "  try:\n",
    "    with open(path, encoding='utf-8') as data:\n",
    "      reader = csv.DictReader(data)\n",
    "      for row in reader:\n",
    "        yield row\n",
    "  except IOError:\n",
    "    print(\"ERROR: file [\" + path + \"] not found/accessible\")\n",
    "    yield \"\"\n",
    "    return\n",
    "  data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(FILE_NAME) as f:\n",
    "        sentiments = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_idx = {}\n",
    "for sentiment in sentiments:\n",
    "    if sentiment['file_name'] not in file_idx.keys():\n",
    "        file_idx[sentiment['file_name']] = []\n",
    "    file_idx[sentiment['file_name']].append(sentiment['file_idx'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [12:24<00:00, 248.04s/it]\n"
     ]
    }
   ],
   "source": [
    "texts_to_vectorize = []\n",
    "predicted_sentiments = []\n",
    "for file_name in tqdm.tqdm(file_idx.keys()):\n",
    "    file_idx[file_name].sort()\n",
    "    csv_path = os.path.join(CSV_FOLDER, file_name)\n",
    "    for idx, row in enumerate(read_csv(csv_path)):\n",
    "        if idx > file_idx[file_name][-1]:\n",
    "            break\n",
    "        elif idx not in file_idx[file_name]:\n",
    "            continue\n",
    "        try:\n",
    "            tx = json.loads(row[\"tx\"])\n",
    "        except Exception as e:\n",
    "            print(\"ERROR: failed to parse row #\" + str(idx) + \" from [\" + csv_path + \"]\")\n",
    "            break\n",
    "        lang = str(row[\"la\"])\n",
    "        if lang == 'en':\n",
    "            continue\n",
    "        whole_text = \"\"\n",
    "        for tx_elem in tx:\n",
    "            for elem in gen_dict_extract('text', tx_elem):\n",
    "                whole_text += elem\n",
    "        whole_text.replace('\\t', '')\n",
    "        whole_text = ' '.join(whole_text.split())\n",
    "        if len(whole_text) == 0:\n",
    "            continue\n",
    "        language = 'german'\n",
    "        tokenizer = ToktokTokenizer()\n",
    "        text_tokens = tokenizer.tokenize(whole_text)\n",
    "        pattern = re.compile('[\\W_]+')\n",
    "        text_tokens = [pattern.sub('', x) for x in text_tokens]\n",
    "        text_tokens = [x for x in text_tokens if x.lower() not in stopwords.words(language) and x.isalnum()]\n",
    "        stemmer = SnowballStemmer(language)\n",
    "        text_tokens = [stemmer.stem(x) for x in text_tokens]\n",
    "        texts_to_vectorize.append(' '.join(text_tokens))\n",
    "        matching_sentiment = [x for x in sentiments if x['file_name'] == file_name and x['file_idx'] == idx][0]\n",
    "        predicted_sentiments.append(matching_sentiment['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13258\n",
      "13258\n"
     ]
    }
   ],
   "source": [
    "print(len(predicted_sentiments))\n",
    "print(len(texts_to_vectorize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(texts_to_vectorize)\n",
    "predicted_sentiments = np.array(predicted_sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k_fold = StratifiedKFold(n_splits=3, shuffle=True)\n",
    "# splits = k_fold.split(vectors, predicted_sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "# from sklearn.gaussian_process.kernels import RBF\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for train_index, test_index in splits:\n",
    "#     X_train, X_test = vectors[train_index], vectors[test_index]\n",
    "#     y_train, y_test = predicted_sentiments[train_index], predicted_sentiments[test_index]\n",
    "#     cls.fit(X=X_train, y=y_train)\n",
    "#     print(cls.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "scores = [\"precision_micro\", \"precision_macro\", \"precision_weighted\", \"recall_micro\", \"recall_macro\",\n",
    "              \"recall_weighted\", \"f1_micro\", \"f1_macro\", \"f1_weighted\", \"accuracy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation = cross_validate(cls, X=vectors, y=predicted_sentiments, cv=k_fold, scoring=scores,\n",
    "#                               return_train_score=True)\n",
    "# for k, v in validation.items():\n",
    "#     print(f'{k}: {v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls.fit(X=vectors, y=predicted_sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(vectorizer, open(\"vocabulary.tfidf\",\"wb\"))\n",
    "pickle.dump(cls, open(\"rf.mdl\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
